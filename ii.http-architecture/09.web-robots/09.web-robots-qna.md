# 질문과 답변

## Q. md5 메시지 요약 함수란?

* md는 message disgest의 약자이다. md 함수는 해시 함수의 일종인데, f\(원본 메시지\) = \(해당 메시지의 요약\)인 것.
* 원본 메시지는 임의의 길이를 가질 수 있지만, 요약인 해시값은 고정 크기를 갖는다.
* md 계열 함수들은 Rivest라는 사람이 만들었으며, md2, md4, md5를 거쳐서 발전했다.
  * SHA\(Secure Hash Algorithm, 안전한 해시 알고리즘\)함수들 역시 md4에 기반한다.
* md2는 초기의 8비트 컴퓨터에 최적화되어 있는 8비트 버전이고, md4, md5는 32비트에 최적화되어 있다.
* 현재는 md5 알고리즘의 암호학적 취약성이 발견되었기 때문에, SHA-1과 같은 다른 알고리즘을 사용할 것이 권장된다.
* 자세한 알고리즘의 동작은 위키백과를 참고할 것.

> ref: 정보통신용어기술해설 [http://www.ktword.co.kr/abbr\_view.php/abbr\_view.php?nav=2&id=960&m\_temp1=1071](http://www.ktword.co.kr/abbr_view.php/abbr_view.php?nav=2&id=960&m_temp1=1071) ref: 위키백과 [https://ko.wikipedia.org/wiki/MD5](https://ko.wikipedia.org/wiki/MD5)

## Q. 가상 호스팅 문제에서 - 요청 URI로도 어떤 docroot에 접근해야 할지 구분할 수 있는 것 아닌가? 왜 무조건 디폴트 루트로 가게 되는 것인가?

URI에 대한 요청이, DNS를 거쳐서 최종적으로는 ip 주소로 변환되 날아가기 때문.

## Q. 유저 에이전트가 로봇인 경우 어떻게 처리할까?

## headless browser

> ref: \(HeadLess란? 부분 참\) [https://beomi.github.io/gb-crawling/posts/2017-09-28-HowToMakeWebCrawler-Headless-Chrome.html](https://beomi.github.io/gb-crawling/posts/2017-09-28-HowToMakeWebCrawler-Headless-Chrome.html)

## Q. 구글의 랭킹 알고리즘에 대해 설명해보자.

> ref: Google의 페이지 랭크 알고리즘 [https://sungmooncho.com/2012/08/26/pagerank/](https://sungmooncho.com/2012/08/26/pagerank/)

### Pagerank

* 기본적인 개념은 논문처럼 피인용수가 높으면 그 페이지는 좋은 페이지라고 생각하는 것이다.
* 단순히 피인용 숫자만 더한다고 생각해보자. Rank\(특정 페이지\) = Rank\(특페를 인용한 페이지1\) + Rank\(특페를 인용한 페이지2\) + Rank\(너무 길다 페이지3\) + ... Rank\(페이지n\).
* 어떤 논문은 논문 1000개를 인용할 수도 있고, 어떤 논문은 논문 1개만 인용했을 수도 있다. 그런데도 각각의 인용 1개가 같은 가중치를 가지나? 아니다.
* Rank를 정규화한다. 정규화한다는 것은 Rank\(페이지\) / \(해당 페이지가 인용한 페이지 갯수\) \(이걸 F\(페이지\)라고 하자\)
* 랭킹이 높은 페이지들이 인용을 많이 할수록 해당 페이지도 랭킹이 올라간다.
* 식이 Rank\(특정 페이지\) = F\(걔 인용한 다른 페이지\) + F\(다른 페이지2\) + ... + F\(다른 페이지3\)
* 근데 사람들이 무한히 클릭만 하고 있지는 않고, 언젠가는 만족하고 멈추지 않는가? 그니까 그 페이지에 링크가 걸려 있어도 꼭 누르지 않을 수도 있다. 그래서 링크 누를 확률인 damping factor라는 것을 곱해준다. \(ref 참고\)

## Q. 로봇이 방문한 URL 탐색을 위해 사용하는 자료구조 중 트리는 어떤식으로 이뤄져 있을까?

## ‌Q. 만약 웹페이지 컨텐츠가 browser history 에 종속적이라면 \(script 파일에 종속적이라면\) 크롤러는 이에 대해 무방비 상태가 될까?

## ‌ Q. 사악한 웹 마스터가 로봇을 함정에 빠트릴 수 있는 함정들은 무엇들이 있을까?

## ‌ Q. 사적 용도\(서치엔진이 아닌데도\)로 로봇을 사용해도 되는가?

## ‌ Q. 느슨한 존재 비트맵 설명만으론 해시 테이블과 크게 다른 것 같지 않은데..

## ‌ Q. 크롤러는 웹 서버가 가상 호스팅을 지원하는지, IP 주소가 같은 장비를 가리키는지 등의 정보를 어떻게 알 수 있나?

## ‌ Q. 페이지 콘텐츠에서 어느 정도 크기의 바이트를 얻어오고 또 어떻게 생성?

## ‌ Q. 현재는 가상호스팅 환경에서 로컬 robots.txt 파일을 개별 서브디렉터리에 설치할 수 있나?

## ‌ Q. 딥 웹의 원리는? 단순히 robots.txt에서 모든 크롤링을 막아놓은 것인가?

